{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517443e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "# DESCRIPTION: \n",
    "#              \n",
    "# RESULTS:     \n",
    "#              \n",
    "##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e744b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# CONFIG & HYPERPARAMS\n",
    "######################\n",
    "\n",
    "class HyperParams:\n",
    "    pass\n",
    "\n",
    "IMG_PATH = \"C:/Projects/VenusDenoise/dataset/cases/256/0100_1000/\"\n",
    "\n",
    "IMG_PATH_VALID = IMG_PATH + \"validation/\"\n",
    "IMG_PATH_TEST = IMG_PATH + \"test/\"\n",
    "IMG_PATH_TRAIN = IMG_PATH\n",
    "\n",
    "hyperparams = HyperParams()\n",
    "hyperparams.IMG_WIDTH = 256\n",
    "hyperparams.IMG_HEIGHT = 256\n",
    "hyperparams.EPOCHS = 600 #10000\n",
    "hyperparams.BATCH_SIZE = 16\n",
    "hyperparams.START_NEURONS = 8 # UNET\n",
    "hyperparams.LOSS = 'mae_nz'\n",
    "\n",
    "IMG_WIDTH = hyperparams.IMG_WIDTH\n",
    "IMG_HEIGHT = hyperparams.IMG_HEIGHT\n",
    "\n",
    "IMG_CASE = str(IMG_WIDTH) +  \"/0100_1000\"\n",
    "MODEL_NAME = \"0100_1000-256-unet-c\"\n",
    "\n",
    "#DEST_TESTS = os.path.abspath(os.path.join('../../../out_tests/', MODEL_NAME))\n",
    "\n",
    "\n",
    "class RadianceLimits:\n",
    "    pass\n",
    "radiance_limits = RadianceLimits()\n",
    "radiance_limits.noisy_min = 0\n",
    "radiance_limits.noisy_max = 0.0898\n",
    "radiance_limits.nitid_min = 0\n",
    "radiance_limits.nitid_max = 0.3248\n",
    "\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "#hyperparams.OPTIMIZER = Adam(learning_rate=0.0001)\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "hyperparams.OPTIMIZER = Nadam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1634ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# IMPORTS\n",
    "##################\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow \n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('../../support/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import DatasetUtilsTifF as dsutils\n",
    "import TrainModelC as train\n",
    "import ReportsK as reports\n",
    "import UnetI_H as model_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b516b30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8675798185035674961\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5387583488\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15887343631349797701\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6\"\n",
      "]\n",
      "Tensorflow version: 2.6.0\n",
      "Keras Version: 2.6.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tf.keras.__version__}\")\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d50ed68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4dda609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 8)  80          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 8)  584         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 128, 128, 8)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128, 128, 8)  0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 16) 1168        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 16) 2320        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 16)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 64, 16)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 32)   4640        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 32)   9248        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 32)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 32)   0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 64)   18496       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 64)   36928       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16, 16, 64)   0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 128)  73856       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 128)  147584      conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 32, 32, 64)   73792       conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 128)  0           conv2d_transpose[0][0]           \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 128)  0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 64)   73792       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 64)   36928       conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 64, 64, 64)   36928       conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 96)   0           conv2d_transpose_1[0][0]         \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64, 64, 96)   0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 32)   27680       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 32)   9248        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 128, 128, 64) 18496       conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 128, 80) 0           conv2d_transpose_2[0][0]         \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128, 128, 80) 0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 128, 128, 16) 11536       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 128, 128, 16) 2320        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 256, 256, 64) 9280        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256, 256, 72) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256, 256, 72) 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 256, 256, 8)  5192        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 256, 256, 8)  584         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 256, 256, 1)  9           conv2d_17[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 600,689\n",
      "Trainable params: 600,689\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'Nadam',\n",
       " 'learning_rate': 0.0001,\n",
       " 'decay': 0.004,\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'epsilon': 1e-07}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################\n",
    "# MODEL DEFINITION\n",
    "##################\n",
    "\n",
    "model = model_factory.buildModel(hyperparams)\n",
    "model.summary()\n",
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf11c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read dataset. Path: C:/Projects/VenusDenoise/dataset/cases/256/0100_1000/\n",
      "Noisy files:606\n",
      "Nitid files:606\n",
      "Read dataset. Path: C:/Projects/VenusDenoise/dataset/cases/256/0100_1000/validation/\n",
      "Noisy files:148\n",
      "Nitid files:148\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# PREPARE DATA\n",
    "##################\n",
    "\n",
    "train_noisy_files, train_nitid_files, train_noisy, train_nitid = dsutils.readDataset( IMG_PATH_TRAIN, hyperparams.IMG_WIDTH, hyperparams.IMG_HEIGHT, radiance_limits)\n",
    "val_noisy_files, val_nitid_files, val_noisy, val_nitid = dsutils.readDataset( IMG_PATH_VALID, hyperparams.IMG_WIDTH, hyperparams.IMG_HEIGHT, radiance_limits)\n",
    "\n",
    "train_noisy, train_nitid = dsutils.reshapeDataset( train_noisy, train_nitid, hyperparams.IMG_WIDTH, hyperparams.IMG_HEIGHT )\n",
    "val_noisy, val_nitid = dsutils.reshapeDataset( val_noisy, val_nitid, hyperparams.IMG_WIDTH, hyperparams.IMG_HEIGHT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b591f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "38/38 [==============================] - 13s 153ms/step - loss: 0.0189 - val_loss: 0.0109\n",
      "Epoch 2/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0095 - val_loss: 0.0104\n",
      "Epoch 3/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 4/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 5/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 6/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 7/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 8/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 9/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 10/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 11/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0115 - val_loss: 0.0114\n",
      "Epoch 12/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 13/600\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 14/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 15/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0116 - val_loss: 0.0115\n",
      "Epoch 16/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0116 - val_loss: 0.0115\n",
      "Epoch 17/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0116 - val_loss: 0.0116\n",
      "Epoch 18/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0116 - val_loss: 0.0116\n",
      "Epoch 19/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0116\n",
      "Epoch 20/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0116\n",
      "Epoch 21/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0116 - val_loss: 0.0116\n",
      "Epoch 22/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0116\n",
      "Epoch 23/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0116\n",
      "Epoch 24/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0116\n",
      "Epoch 25/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0116\n",
      "Epoch 26/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0116\n",
      "Epoch 27/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 28/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 29/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 30/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 31/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 32/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 33/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 34/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 35/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 36/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 37/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 38/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 39/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 40/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 41/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 42/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 43/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 44/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 45/600\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 46/600\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 47/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 48/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 49/600\n",
      "38/38 [==============================] - 4s 99ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 50/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 51/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 52/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 53/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 54/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 55/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 56/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 57/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 58/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 59/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 60/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 61/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 62/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 63/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 64/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 65/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 66/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 67/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 68/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 69/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 70/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 71/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 72/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 73/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 74/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 75/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 76/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 77/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 78/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 79/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 80/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 81/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 82/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 4s 99ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 83/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 84/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 85/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 86/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 87/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 88/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 89/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 90/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 91/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 92/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 93/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 94/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 95/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0117 - val_loss: 0.0118\n",
      "Epoch 96/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 97/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 98/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 99/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 100/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 101/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 102/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 103/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 104/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 105/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 106/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 107/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 108/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 109/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 110/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 111/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 112/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 113/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 114/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 115/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 116/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 117/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 118/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 119/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 120/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 121/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 122/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 123/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 124/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 125/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 126/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 127/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 128/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 129/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 130/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 131/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 132/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 133/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 134/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 135/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 136/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 137/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 138/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 139/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 140/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 141/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 142/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 143/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 144/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 145/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 146/600\n",
      "38/38 [==============================] - 4s 99ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 147/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 148/600\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 149/600\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 150/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 151/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 152/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 153/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 154/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 155/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 156/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 157/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 158/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 159/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 160/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 161/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 162/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 163/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 164/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 165/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 166/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 167/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 168/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 169/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 170/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 171/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 172/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 173/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 174/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 175/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 176/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 177/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 178/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 179/600\n",
      "38/38 [==============================] - 4s 99ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 180/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 181/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 182/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 183/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 184/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 185/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 186/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 187/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 188/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 189/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 190/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 191/600\n",
      "38/38 [==============================] - 4s 99ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 192/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 193/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 194/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 195/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 196/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 197/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 198/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 199/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 200/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 201/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 202/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 203/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 204/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 205/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 206/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 207/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 208/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 209/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 210/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 211/600\n",
      "38/38 [==============================] - 4s 99ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 212/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 213/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 214/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 215/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 216/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 217/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 218/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 219/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 220/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 221/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 222/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 223/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 224/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 225/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 226/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 227/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 228/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 229/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 230/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 231/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 232/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 233/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 234/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 235/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 236/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 237/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 238/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 239/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 240/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 241/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 242/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 243/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 244/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 245/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 246/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 247/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 248/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 249/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 250/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 251/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 252/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 253/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 254/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 255/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 256/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 257/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 258/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 259/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 260/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 261/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 262/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 263/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 264/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 265/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 266/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 267/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 268/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 269/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 270/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 271/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 272/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 273/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 274/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 275/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 276/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 277/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 278/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 279/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 280/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 281/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 282/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 283/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 284/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 285/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 286/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 287/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 288/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 289/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 290/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 291/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 292/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 293/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 294/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 295/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 296/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 297/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 298/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 299/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 300/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 301/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 302/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 303/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 304/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 305/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 306/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 307/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 308/600\n",
      "38/38 [==============================] - 4s 99ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 309/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 310/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 311/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 312/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 313/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 314/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 315/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 316/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 317/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 318/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 319/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 320/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 321/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 322/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 323/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 324/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 325/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 326/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 327/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 328/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 329/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 330/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 331/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 332/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 333/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 334/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 335/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 336/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 337/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 338/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 339/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 340/600\n",
      "38/38 [==============================] - 4s 99ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 341/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 342/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 343/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 344/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 345/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 346/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 347/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 348/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 349/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 350/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 351/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 352/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 353/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 354/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 355/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 356/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 357/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 358/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 359/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 360/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 361/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 362/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 363/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 364/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 365/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 366/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 367/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 368/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 369/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 370/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 371/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 372/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 373/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 374/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 375/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 376/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 377/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 378/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 379/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 380/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 381/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 382/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 383/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 384/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 385/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 386/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 387/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 388/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 389/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 390/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 391/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 392/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 393/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 394/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 395/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 396/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 397/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 398/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 399/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 400/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 401/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 402/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 403/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 404/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 405/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 406/600\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 407/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 408/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 409/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 410/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 411/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 412/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 413/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 414/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 415/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 416/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 417/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 418/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 419/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 420/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 421/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 422/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 423/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 424/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 425/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 426/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 427/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 428/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 429/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 430/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 431/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 432/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 433/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 434/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 435/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 436/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 437/600\n",
      "38/38 [==============================] - 4s 99ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 438/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 439/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 440/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 441/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 442/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 443/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 444/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 445/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 446/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 447/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 448/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 449/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 450/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 451/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 452/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 453/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 454/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 455/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 456/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 457/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 458/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 459/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 460/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 461/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 462/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 463/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 464/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 465/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 466/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 467/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 468/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 469/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 470/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 471/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 472/600\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 473/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 474/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 475/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 476/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 477/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 478/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 479/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 480/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 481/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 482/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 483/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 484/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 485/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 486/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 487/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 488/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 489/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 490/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 491/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 492/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 493/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 494/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 495/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 496/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 497/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 498/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 499/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 500/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 501/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 502/600\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Train size:606\n",
      "Valid.size:148\n",
      "--- 1877.549216747284 seconds ---\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# TRAIN MODEL\n",
    "##################\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.compat.v1.keras.backend import set_session\n",
    "#config = tf.compat.v1.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "#config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "#sess = tf.compat.v1.Session(config=config)\n",
    "#set_session(sess)\n",
    "\n",
    "\n",
    "hist = train.fit( model, hyperparams, train_noisy, train_nitid, val_noisy, val_nitid, patience = 500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8148067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################\n",
    "# REPORTS\n",
    "##################\n",
    "\n",
    "reports.plotHistory( hist )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f9e4309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Projects\\VenusDenoise\\saves\\0100_1000-256-unet-c\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(os.path.abspath(os.path.join('../../../saves/', MODEL_NAME)), model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
