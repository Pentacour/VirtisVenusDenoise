{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1420dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "# DESCRIPTION: \n",
    "# RESULTS:     \n",
    "#              \n",
    "##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "144dbf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# CONFIG & HYPERPARAMS\n",
    "######################\n",
    "\n",
    "import os\n",
    "\n",
    "class HyperParams:\n",
    "    pass\n",
    "\n",
    "IMG_PATH = \"C:/Projects/VenusDenoise/dataset/cases/64/0100_1000/\"\n",
    "\n",
    "IMG_PATH_VALID = IMG_PATH + \"validation/\"\n",
    "IMG_PATH_TEST = IMG_PATH + \"test/\"\n",
    "IMG_PATH_TRAIN = IMG_PATH\n",
    "\n",
    "hyperparams = HyperParams()\n",
    "hyperparams.IMG_WIDTH = 64\n",
    "hyperparams.IMG_HEIGHT = 64\n",
    "hyperparams.EPOCHS = 320\n",
    "hyperparams.BATCH_SIZE = 32\n",
    "\n",
    "hyperparams.LOSS = 'mean_absolute_error'\n",
    "\n",
    "IMG_WIDTH = hyperparams.IMG_WIDTH\n",
    "IMG_HEIGHT = hyperparams.IMG_HEIGHT\n",
    "\n",
    "IMG_CASE = str(IMG_WIDTH) +  \"/0100_1000\"\n",
    "MODEL_NAME = \"0100_1000-64-convsim-h\"\n",
    "\n",
    "DEST_TESTS = os.path.abspath(os.path.join('../../../out_tests/', MODEL_NAME))\n",
    "\n",
    "class RadianceLimits:\n",
    "    pass\n",
    "radiance_limits = RadianceLimits()\n",
    "radiance_limits.noisy_min = 0\n",
    "radiance_limits.noisy_max = 0.0898\n",
    "radiance_limits.nitid_min = 0\n",
    "radiance_limits.nitid_max = 0.3248\n",
    "\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "hyperparams.OPTIMIZER = Nadam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be827de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# IMPORTS\n",
    "##################\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow \n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('../../support/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import DatasetUtilsTifF as dsutils\n",
    "import TrainModelC as train\n",
    "import ReportsK as reports\n",
    "import ConvSimG as model_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a22f1b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 32)   544         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 16)   8208        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 16)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 8, 8, 8)      2056        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 32)   544         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 8, 8, 8)      0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 16, 16, 16)   2064        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 16)   8208        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 16)   0           conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 16, 16)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 16, 16)   0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 32, 32, 32)   8224        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 32, 32, 32)   8224        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 32)   0           conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 32)   0           conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 32)   0           activation[0][0]                 \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 32)   0           activation_4[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 64, 64, 1)    513         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 64, 64, 1)    513         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 1)    0           conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 1)    0           conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 64, 1)    0           activation_3[0][0]               \n",
      "                                                                 activation_9[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 39,098\n",
      "Trainable params: 39,098\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'Nadam',\n",
       " 'learning_rate': 0.0001,\n",
       " 'decay': 0.004,\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'epsilon': 1e-07}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################\n",
    "# MODEL DEFINITION\n",
    "##################\n",
    "model = model_factory.buildModel(hyperparams)\n",
    "model.summary()\n",
    "model.optimizer.get_config()\n",
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f1de709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read dataset. Path: C:/Projects/VenusDenoise/dataset/cases/64/0100_1000/\n",
      "Noisy files:8886\n",
      "Nitid files:8886\n",
      "Read dataset. Path: C:/Projects/VenusDenoise/dataset/cases/64/0100_1000/validation/\n",
      "Noisy files:2209\n",
      "Nitid files:2209\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# PREPARE DATA\n",
    "##################\n",
    "\n",
    "train_noisy_files, train_nitid_files, train_noisy, train_nitid = dsutils.readDataset( IMG_PATH_TRAIN, hyperparams.IMG_WIDTH, hyperparams.IMG_HEIGHT, radiance_limits)\n",
    "val_noisy_files, val_nitid_files, val_noisy, val_nitid = dsutils.readDataset( IMG_PATH_VALID, hyperparams.IMG_WIDTH, hyperparams.IMG_HEIGHT, radiance_limits)\n",
    "\n",
    "train_noisy, train_nitid = dsutils.reshapeDataset( train_noisy, train_nitid, hyperparams.IMG_WIDTH, hyperparams.IMG_HEIGHT )\n",
    "val_noisy, val_nitid = dsutils.reshapeDataset( val_noisy, val_nitid, hyperparams.IMG_WIDTH, hyperparams.IMG_HEIGHT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1df4c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6\n",
      "\n",
      "Epoch 1/320\n",
      "278/278 [==============================] - 8s 17ms/step - loss: 0.0696 - val_loss: 0.0300\n",
      "Epoch 2/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0319 - val_loss: 0.0258\n",
      "Epoch 3/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0294 - val_loss: 0.0249\n",
      "Epoch 4/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0282 - val_loss: 0.0231\n",
      "Epoch 5/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0276 - val_loss: 0.0227\n",
      "Epoch 6/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0271 - val_loss: 0.0223\n",
      "Epoch 7/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0269 - val_loss: 0.0221\n",
      "Epoch 8/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0266 - val_loss: 0.0221\n",
      "Epoch 9/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0264 - val_loss: 0.0216\n",
      "Epoch 10/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0262 - val_loss: 0.0213\n",
      "Epoch 11/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0260 - val_loss: 0.0216\n",
      "Epoch 12/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0259 - val_loss: 0.0220\n",
      "Epoch 13/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0258 - val_loss: 0.0228\n",
      "Epoch 14/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0256 - val_loss: 0.0221\n",
      "Epoch 15/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0255 - val_loss: 0.0211\n",
      "Epoch 16/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0254 - val_loss: 0.0213\n",
      "Epoch 17/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0253 - val_loss: 0.0217\n",
      "Epoch 18/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0252 - val_loss: 0.0224\n",
      "Epoch 19/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0251 - val_loss: 0.0232\n",
      "Epoch 20/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0250 - val_loss: 0.0215\n",
      "Epoch 21/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0250 - val_loss: 0.0211\n",
      "Epoch 22/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0249 - val_loss: 0.0218\n",
      "Epoch 23/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0248 - val_loss: 0.0216\n",
      "Epoch 24/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0248 - val_loss: 0.0215\n",
      "Epoch 25/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0247 - val_loss: 0.0222\n",
      "Epoch 26/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0246 - val_loss: 0.0217\n",
      "Epoch 27/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0246 - val_loss: 0.0223\n",
      "Epoch 28/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0245 - val_loss: 0.0232\n",
      "Epoch 29/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0244 - val_loss: 0.0225\n",
      "Epoch 30/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0243 - val_loss: 0.0213\n",
      "Epoch 31/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0243 - val_loss: 0.0213\n",
      "Epoch 32/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0243 - val_loss: 0.0227\n",
      "Epoch 33/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0242 - val_loss: 0.0220\n",
      "Epoch 34/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0241 - val_loss: 0.0220\n",
      "Epoch 35/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0241 - val_loss: 0.0227\n",
      "Epoch 36/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0240 - val_loss: 0.0218\n",
      "Epoch 37/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0240 - val_loss: 0.0231\n",
      "Epoch 38/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0239 - val_loss: 0.0228\n",
      "Epoch 39/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0240 - val_loss: 0.0215\n",
      "Epoch 40/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0239 - val_loss: 0.0230\n",
      "Epoch 41/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0238 - val_loss: 0.0215\n",
      "Epoch 42/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0238 - val_loss: 0.0217\n",
      "Epoch 43/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0237 - val_loss: 0.0230\n",
      "Epoch 44/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0238 - val_loss: 0.0227\n",
      "Epoch 45/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0237 - val_loss: 0.0227\n",
      "Epoch 46/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0236 - val_loss: 0.0228\n",
      "Epoch 47/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0236 - val_loss: 0.0229\n",
      "Epoch 48/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0236 - val_loss: 0.0218\n",
      "Epoch 49/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0235 - val_loss: 0.0220\n",
      "Epoch 50/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0235 - val_loss: 0.0223\n",
      "Epoch 51/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0235 - val_loss: 0.0246\n",
      "Epoch 52/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0235 - val_loss: 0.0235\n",
      "Epoch 53/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0234 - val_loss: 0.0229\n",
      "Epoch 54/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0234 - val_loss: 0.0223\n",
      "Epoch 55/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0234 - val_loss: 0.0221\n",
      "Epoch 56/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0234 - val_loss: 0.0224\n",
      "Epoch 57/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0233 - val_loss: 0.0237\n",
      "Epoch 58/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0232 - val_loss: 0.0226\n",
      "Epoch 59/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0233 - val_loss: 0.0216\n",
      "Epoch 60/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0232 - val_loss: 0.0225\n",
      "Epoch 61/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0232 - val_loss: 0.0242\n",
      "Epoch 62/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0232 - val_loss: 0.0221\n",
      "Epoch 63/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0232 - val_loss: 0.0225\n",
      "Epoch 64/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0232 - val_loss: 0.0245\n",
      "Epoch 65/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0231 - val_loss: 0.0237\n",
      "Epoch 66/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0231 - val_loss: 0.0224\n",
      "Epoch 67/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0231 - val_loss: 0.0238\n",
      "Epoch 68/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0231 - val_loss: 0.0225\n",
      "Epoch 69/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0230 - val_loss: 0.0237\n",
      "Epoch 70/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0231 - val_loss: 0.0240\n",
      "Epoch 71/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0230 - val_loss: 0.0217\n",
      "Epoch 72/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0230 - val_loss: 0.0240\n",
      "Epoch 73/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0230 - val_loss: 0.0227\n",
      "Epoch 74/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0229 - val_loss: 0.0227\n",
      "Epoch 75/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0230 - val_loss: 0.0228\n",
      "Epoch 76/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0230 - val_loss: 0.0233\n",
      "Epoch 77/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0229 - val_loss: 0.0238\n",
      "Epoch 78/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0229 - val_loss: 0.0227\n",
      "Epoch 79/320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0229 - val_loss: 0.0228\n",
      "Epoch 80/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0229 - val_loss: 0.0229\n",
      "Epoch 81/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0229 - val_loss: 0.0222\n",
      "Epoch 82/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0228 - val_loss: 0.0229\n",
      "Epoch 83/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0228 - val_loss: 0.0232\n",
      "Epoch 84/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0228 - val_loss: 0.0229\n",
      "Epoch 85/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0228 - val_loss: 0.0218\n",
      "Epoch 86/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0228 - val_loss: 0.0227\n",
      "Epoch 87/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0227 - val_loss: 0.0224\n",
      "Epoch 88/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0227 - val_loss: 0.0243\n",
      "Epoch 89/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0227 - val_loss: 0.0232\n",
      "Epoch 90/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0227 - val_loss: 0.0222\n",
      "Epoch 91/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0227 - val_loss: 0.0219\n",
      "Epoch 92/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0227 - val_loss: 0.0246\n",
      "Epoch 93/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0227 - val_loss: 0.0228\n",
      "Epoch 94/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0226 - val_loss: 0.0240\n",
      "Epoch 95/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0226 - val_loss: 0.0226\n",
      "Epoch 96/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0226 - val_loss: 0.0243\n",
      "Epoch 97/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0226 - val_loss: 0.0253\n",
      "Epoch 98/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0226 - val_loss: 0.0235\n",
      "Epoch 99/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0226 - val_loss: 0.0231\n",
      "Epoch 100/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0226 - val_loss: 0.0225\n",
      "Epoch 101/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0225 - val_loss: 0.0220\n",
      "Epoch 102/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0225 - val_loss: 0.0224\n",
      "Epoch 103/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0226 - val_loss: 0.0236\n",
      "Epoch 104/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0225 - val_loss: 0.0227\n",
      "Epoch 105/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0225 - val_loss: 0.0220\n",
      "Epoch 106/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0225 - val_loss: 0.0243\n",
      "Epoch 107/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0225 - val_loss: 0.0239\n",
      "Epoch 108/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0224 - val_loss: 0.0221\n",
      "Epoch 109/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0224 - val_loss: 0.0236\n",
      "Epoch 110/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0224 - val_loss: 0.0241\n",
      "Epoch 111/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0224 - val_loss: 0.0227\n",
      "Epoch 112/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0224 - val_loss: 0.0229\n",
      "Epoch 113/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0224 - val_loss: 0.0223\n",
      "Epoch 114/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0223 - val_loss: 0.0231\n",
      "Epoch 115/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0223 - val_loss: 0.0224\n",
      "Epoch 116/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0223 - val_loss: 0.0238\n",
      "Epoch 117/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0224 - val_loss: 0.0243\n",
      "Epoch 118/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0224 - val_loss: 0.0243\n",
      "Epoch 119/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0223 - val_loss: 0.0246\n",
      "Epoch 120/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0223 - val_loss: 0.0227\n",
      "Epoch 121/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0223 - val_loss: 0.0228\n",
      "Epoch 122/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0222 - val_loss: 0.0229\n",
      "Epoch 123/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0222 - val_loss: 0.0235\n",
      "Epoch 124/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0222 - val_loss: 0.0223\n",
      "Epoch 125/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0222 - val_loss: 0.0240\n",
      "Epoch 126/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0222 - val_loss: 0.0232\n",
      "Epoch 127/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0222 - val_loss: 0.0235\n",
      "Epoch 128/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0222 - val_loss: 0.0230\n",
      "Epoch 129/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0221 - val_loss: 0.0239\n",
      "Epoch 130/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0221 - val_loss: 0.0228\n",
      "Epoch 131/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0222 - val_loss: 0.0226\n",
      "Epoch 132/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0221 - val_loss: 0.0242\n",
      "Epoch 133/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0221 - val_loss: 0.0227\n",
      "Epoch 134/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0221 - val_loss: 0.0235\n",
      "Epoch 135/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0221 - val_loss: 0.0226\n",
      "Epoch 136/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0220 - val_loss: 0.0223\n",
      "Epoch 137/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0220 - val_loss: 0.0232\n",
      "Epoch 138/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0220 - val_loss: 0.0229\n",
      "Epoch 139/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0220 - val_loss: 0.0233\n",
      "Epoch 140/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0220 - val_loss: 0.0228\n",
      "Epoch 141/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0220 - val_loss: 0.0230\n",
      "Epoch 142/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0220 - val_loss: 0.0241\n",
      "Epoch 143/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0219 - val_loss: 0.0229\n",
      "Epoch 144/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0219 - val_loss: 0.0226\n",
      "Epoch 145/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0219 - val_loss: 0.0224\n",
      "Epoch 146/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0219 - val_loss: 0.0232\n",
      "Epoch 147/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0219 - val_loss: 0.0230\n",
      "Epoch 148/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0219 - val_loss: 0.0249\n",
      "Epoch 149/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0219 - val_loss: 0.0232\n",
      "Epoch 150/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0217 - val_loss: 0.0225\n",
      "Epoch 151/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0218 - val_loss: 0.0238\n",
      "Epoch 152/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0218 - val_loss: 0.0228\n",
      "Epoch 153/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0218 - val_loss: 0.0232\n",
      "Epoch 154/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0218 - val_loss: 0.0244\n",
      "Epoch 155/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0218 - val_loss: 0.0248\n",
      "Epoch 156/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0218 - val_loss: 0.0227\n",
      "Epoch 157/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0217 - val_loss: 0.0239\n",
      "Epoch 158/320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0216 - val_loss: 0.0244\n",
      "Epoch 159/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0217 - val_loss: 0.0226\n",
      "Epoch 160/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0217 - val_loss: 0.0222\n",
      "Epoch 161/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0218 - val_loss: 0.0234\n",
      "Epoch 162/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0217 - val_loss: 0.0240\n",
      "Epoch 163/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0217 - val_loss: 0.0227\n",
      "Epoch 164/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0217 - val_loss: 0.0235\n",
      "Epoch 165/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0216 - val_loss: 0.0234\n",
      "Epoch 166/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0216 - val_loss: 0.0233\n",
      "Epoch 167/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0216 - val_loss: 0.0233\n",
      "Epoch 168/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0216 - val_loss: 0.0224\n",
      "Epoch 169/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0216 - val_loss: 0.0219\n",
      "Epoch 170/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0215 - val_loss: 0.0225\n",
      "Epoch 171/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0216 - val_loss: 0.0237\n",
      "Epoch 172/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0216 - val_loss: 0.0224\n",
      "Epoch 173/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0215 - val_loss: 0.0225\n",
      "Epoch 174/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0216 - val_loss: 0.0230\n",
      "Epoch 175/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0215 - val_loss: 0.0228\n",
      "Epoch 176/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0215 - val_loss: 0.0224\n",
      "Epoch 177/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0214 - val_loss: 0.0234\n",
      "Epoch 178/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0215 - val_loss: 0.0228\n",
      "Epoch 179/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0215 - val_loss: 0.0221\n",
      "Epoch 180/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0214 - val_loss: 0.0228\n",
      "Epoch 181/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0214 - val_loss: 0.0233\n",
      "Epoch 182/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0214 - val_loss: 0.0241\n",
      "Epoch 183/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0214 - val_loss: 0.0237\n",
      "Epoch 184/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0214 - val_loss: 0.0224\n",
      "Epoch 185/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0214 - val_loss: 0.0229\n",
      "Epoch 186/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0213 - val_loss: 0.0223\n",
      "Epoch 187/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0212 - val_loss: 0.0228\n",
      "Epoch 188/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0213 - val_loss: 0.0229\n",
      "Epoch 189/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0213 - val_loss: 0.0226\n",
      "Epoch 190/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0213 - val_loss: 0.0224\n",
      "Epoch 191/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0213 - val_loss: 0.0232\n",
      "Epoch 192/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0213 - val_loss: 0.0218\n",
      "Epoch 193/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0213 - val_loss: 0.0236\n",
      "Epoch 194/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0213 - val_loss: 0.0223\n",
      "Epoch 195/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0213 - val_loss: 0.0241\n",
      "Epoch 196/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0212 - val_loss: 0.0227\n",
      "Epoch 197/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0212 - val_loss: 0.0227\n",
      "Epoch 198/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0212 - val_loss: 0.0229\n",
      "Epoch 199/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0213 - val_loss: 0.0236\n",
      "Epoch 200/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0212 - val_loss: 0.0231\n",
      "Epoch 201/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0212 - val_loss: 0.0238\n",
      "Epoch 202/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0212 - val_loss: 0.0224\n",
      "Epoch 203/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0211 - val_loss: 0.0231\n",
      "Epoch 204/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0212 - val_loss: 0.0230\n",
      "Epoch 205/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0211 - val_loss: 0.0227\n",
      "Epoch 206/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0211 - val_loss: 0.0231\n",
      "Epoch 207/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0211 - val_loss: 0.0229\n",
      "Epoch 208/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0212 - val_loss: 0.0229\n",
      "Epoch 209/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0212 - val_loss: 0.0228\n",
      "Epoch 210/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0210 - val_loss: 0.0224\n",
      "Epoch 211/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0211 - val_loss: 0.0222\n",
      "Epoch 212/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0211 - val_loss: 0.0234\n",
      "Epoch 213/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0211 - val_loss: 0.0232\n",
      "Epoch 214/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0210 - val_loss: 0.0223\n",
      "Epoch 215/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0211 - val_loss: 0.0227\n",
      "Epoch 216/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0210 - val_loss: 0.0229\n",
      "Epoch 217/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0210 - val_loss: 0.0220\n",
      "Epoch 218/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0210 - val_loss: 0.0232\n",
      "Epoch 219/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0210 - val_loss: 0.0233\n",
      "Epoch 220/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0211 - val_loss: 0.0223\n",
      "Epoch 221/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0209 - val_loss: 0.0241\n",
      "Epoch 222/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0210 - val_loss: 0.0239\n",
      "Epoch 223/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0210 - val_loss: 0.0222\n",
      "Epoch 224/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0210 - val_loss: 0.0224\n",
      "Epoch 225/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0210 - val_loss: 0.0225\n",
      "Epoch 226/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0210 - val_loss: 0.0229\n",
      "Epoch 227/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0210 - val_loss: 0.0230\n",
      "Epoch 228/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0210 - val_loss: 0.0218\n",
      "Epoch 229/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0209 - val_loss: 0.0240\n",
      "Epoch 230/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0209 - val_loss: 0.0228\n",
      "Epoch 231/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0209 - val_loss: 0.0230\n",
      "Epoch 232/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0209 - val_loss: 0.0227\n",
      "Epoch 233/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0209 - val_loss: 0.0228\n",
      "Epoch 234/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0209 - val_loss: 0.0228\n",
      "Epoch 235/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0209 - val_loss: 0.0229\n",
      "Epoch 236/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0209 - val_loss: 0.0223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0208 - val_loss: 0.0228\n",
      "Epoch 238/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0209 - val_loss: 0.0226\n",
      "Epoch 239/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0208 - val_loss: 0.0222\n",
      "Epoch 240/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0209 - val_loss: 0.0230\n",
      "Epoch 241/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0208 - val_loss: 0.0227\n",
      "Epoch 242/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0209 - val_loss: 0.0219\n",
      "Epoch 243/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0208 - val_loss: 0.0227\n",
      "Epoch 244/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0208 - val_loss: 0.0224\n",
      "Epoch 245/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0208 - val_loss: 0.0229\n",
      "Epoch 246/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0208 - val_loss: 0.0221\n",
      "Epoch 247/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0208 - val_loss: 0.0218\n",
      "Epoch 248/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0208 - val_loss: 0.0221\n",
      "Epoch 249/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0207 - val_loss: 0.0218\n",
      "Epoch 250/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0208 - val_loss: 0.0221\n",
      "Epoch 251/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0207 - val_loss: 0.0235\n",
      "Epoch 252/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0208 - val_loss: 0.0225\n",
      "Epoch 253/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0207 - val_loss: 0.0226\n",
      "Epoch 254/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0208 - val_loss: 0.0222\n",
      "Epoch 255/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0207 - val_loss: 0.0219\n",
      "Epoch 256/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0207 - val_loss: 0.0240\n",
      "Epoch 257/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0207 - val_loss: 0.0217\n",
      "Epoch 258/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0207 - val_loss: 0.0228\n",
      "Epoch 259/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0207 - val_loss: 0.0228\n",
      "Epoch 260/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0207 - val_loss: 0.0218\n",
      "Epoch 261/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0207 - val_loss: 0.0225\n",
      "Epoch 262/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0206 - val_loss: 0.0224\n",
      "Epoch 263/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0207 - val_loss: 0.0219\n",
      "Epoch 264/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0206 - val_loss: 0.0226\n",
      "Epoch 265/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0207 - val_loss: 0.0225\n",
      "Epoch 266/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0207 - val_loss: 0.0222\n",
      "Epoch 267/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0207 - val_loss: 0.0219\n",
      "Epoch 268/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0208 - val_loss: 0.0222\n",
      "Epoch 269/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0207 - val_loss: 0.0229\n",
      "Epoch 270/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0207 - val_loss: 0.0223\n",
      "Epoch 271/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0207 - val_loss: 0.0227\n",
      "Epoch 272/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0206 - val_loss: 0.0219\n",
      "Epoch 273/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0206 - val_loss: 0.0217\n",
      "Epoch 274/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0206 - val_loss: 0.0236\n",
      "Epoch 275/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0206 - val_loss: 0.0231\n",
      "Epoch 276/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0206 - val_loss: 0.0218\n",
      "Epoch 277/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0206 - val_loss: 0.0215\n",
      "Epoch 278/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0206 - val_loss: 0.0216\n",
      "Epoch 279/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0205 - val_loss: 0.0222\n",
      "Epoch 280/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0205 - val_loss: 0.0224\n",
      "Epoch 281/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0205 - val_loss: 0.0224\n",
      "Epoch 282/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0205 - val_loss: 0.0223\n",
      "Epoch 283/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0205 - val_loss: 0.0227\n",
      "Epoch 284/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0205 - val_loss: 0.0217\n",
      "Epoch 285/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0205 - val_loss: 0.0230\n",
      "Epoch 286/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0205 - val_loss: 0.0216\n",
      "Epoch 287/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0204 - val_loss: 0.0223\n",
      "Epoch 288/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0206 - val_loss: 0.0216\n",
      "Epoch 289/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0206 - val_loss: 0.0223\n",
      "Epoch 290/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0205 - val_loss: 0.0210\n",
      "Epoch 291/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0205 - val_loss: 0.0222\n",
      "Epoch 292/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0205 - val_loss: 0.0219\n",
      "Epoch 293/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0205 - val_loss: 0.0215\n",
      "Epoch 294/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0204 - val_loss: 0.0228\n",
      "Epoch 295/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0205 - val_loss: 0.0226\n",
      "Epoch 296/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0204 - val_loss: 0.0217\n",
      "Epoch 297/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0204 - val_loss: 0.0219\n",
      "Epoch 298/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0204 - val_loss: 0.0216\n",
      "Epoch 299/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0204 - val_loss: 0.0218\n",
      "Epoch 300/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0205 - val_loss: 0.0216\n",
      "Epoch 301/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0204 - val_loss: 0.0212\n",
      "Epoch 302/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0204 - val_loss: 0.0217\n",
      "Epoch 303/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0204 - val_loss: 0.0219\n",
      "Epoch 304/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0204 - val_loss: 0.0218\n",
      "Epoch 305/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0204 - val_loss: 0.0224\n",
      "Epoch 306/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0204 - val_loss: 0.0217\n",
      "Epoch 307/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0204 - val_loss: 0.0219\n",
      "Epoch 308/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0203 - val_loss: 0.0223\n",
      "Epoch 309/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0204 - val_loss: 0.0217\n",
      "Epoch 310/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0205 - val_loss: 0.0214\n",
      "Epoch 311/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0204 - val_loss: 0.0213\n",
      "Epoch 312/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0203 - val_loss: 0.0211\n",
      "Epoch 313/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0203 - val_loss: 0.0223\n",
      "Epoch 314/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0203 - val_loss: 0.0223\n",
      "Epoch 315/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0203 - val_loss: 0.0222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 316/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0203 - val_loss: 0.0219\n",
      "Epoch 317/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0203 - val_loss: 0.0219\n",
      "Epoch 318/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0203 - val_loss: 0.0223\n",
      "Epoch 319/320\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 0.0204 - val_loss: 0.0218\n",
      "Epoch 320/320\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.0203 - val_loss: 0.0214\n",
      "Train size:8886\n",
      "Valid.size:2209\n",
      "--- 1380.8988146781921 seconds ---\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# TRAIN MODEL\n",
    "##################\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "\n",
    "hist = train.fit( model, hyperparams, train_noisy, train_nitid, val_noisy, val_nitid )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e98d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA8R0lEQVR4nO3deXhU5dn48e89k30PJGwJq7KjsokodbcKuFCrVdy11uVVa21rq7b9dbWtb98uarXiUq1Wi2u1aFFcQVxAFtkR2QKEAFkg+548vz/uM5lJmISADEng/lxXrpk5y5x7zmTOfZ7lPEeccxhjjDEt+To6AGOMMZ2TJQhjjDFhWYIwxhgTliUIY4wxYVmCMMYYE5YlCGOMMWFZgjDmIBCRf4jIve1cNkdEzvqq72NMpFmCMMYYE5YlCGOMMWFZgjBHDK9q50ciskJEKkTk7yLSU0TeFJEyEXlXRNJDlr9ARFaLSLGIzBWR4SHzxojIUm+9F4C4Fts6T0SWeet+IiLHHmDMN4jIBhHZLSKzRKSPN11E5C8iki8iJd5nGuXNmyoia7zYtovInQe0w8wRzxKEOdJcBHwdGAKcD7wJ/ATIQH8PtwOIyBBgJnAHkAnMBl4XkRgRiQFeA/4JdANe8t4Xb92xwJPATUB34FFglojE7k+gInIG8HvgEqA3sAV43pt9NnCK9znSgEuBIm/e34GbnHPJwCjg/f3ZrjEBliDMkeavzrldzrntwHxgoXPuc+dcDfAqMMZb7lLgv865d5xzdcAfgXjgJGAiEA3c75yrc869DCwK2cYNwKPOuYXOuQbn3NNAjbfe/rgCeNI5t9SL7x7gRBEZANQBycAwQJxza51zO7z16oARIpLinNvjnFu6n9s1BrAEYY48u0KeV4V5neQ974OesQPgnGsEtgFZ3rztrvlIl1tCnvcHfuhVLxWLSDHQ11tvf7SMoRwtJWQ5594HHgIeBnaJyGMikuItehEwFdgiIvNE5MT93K4xgCUIY1qThx7oAa3zRw/y24EdQJY3LaBfyPNtwG+dc2khfwnOuZlfMYZEtMpqO4Bz7kHn3DhgJFrV9CNv+iLn3DSgB1oV9uJ+btcYwBKEMa15EThXRM4UkWjgh2g10SfAp0A9cLuIRInIN4EJIes+DtwsIid4jcmJInKuiCTvZwz/Aq4TkdFe+8Xv0CqxHBE53nv/aKACqAYavDaSK0Qk1asaKwUavsJ+MEcwSxDGhOGcWwdcCfwVKEQbtM93ztU652qBbwLXAnvQ9op/h6y7GG2HeMibv8Fbdn9jeA/4f8AraKnlKGC6NzsFTUR70GqoIrSdBOAqIEdESoGbvc9hzH4Tu2GQMcaYcKwEYYwxJixLEMYYY8KyBGGMMSYsSxDGGGPCiuroAA6mjIwMN2DAgI4OwxhjuowlS5YUOucyw807rBLEgAEDWLx4cUeHYYwxXYaIbGltnlUxGWOMCcsShDHGmLAsQRhjjAnrsGqDCKeuro7c3Fyqq6s7OpSIiouLIzs7m+jo6I4OxRhzmIhoghCRycADgB94wjl3X4v54s2fClQC1zrnlorIUOCFkEUHAT93zt2/vzHk5uaSnJzMgAEDaD745uHDOUdRURG5ubkMHDiwo8MxxhwmIlbFJCJ+dKz6KcAI4DIRGdFisSnAYO/vRuAR0IHSnHOjnXOjgXFo8nj1QOKorq6me/fuh21yABARunfvftiXkowxh1Yk2yAmABucc5u80S+fB6a1WGYa8IxTC4A0EendYpkzgY3OuVa7Yu3L4ZwcAo6Ez2iMObQimSCy0BunBOR60/Z3menovYHDEpEbRWSxiCwuKCg4oEB3lVZTVl13QOsaY8zhKpIJItwpbcuxxdtcxrs5/AXoTeHDcs495pwb75wbn5kZ9mLAfSooq6G8pv6A1t2X4uJi/va3v+33elOnTqW4uPjgB2SMMe0UyQSRi96iMSAbvYXi/iwzBVjqnNtFhEXqthitJYiGhrZv8jV79mzS0tIiE5QxxrRDJBPEImCwiAz0SgLTgVktlpkFXO3dlnEiUOKc2xEy/zLaqF46WCJZe3/33XezceNGRo8ezfHHH8/pp5/O5ZdfzjHHHAPAN77xDcaNG8fIkSN57LHHmtYbMGAAhYWF5OTkMHz4cG644QZGjhzJ2WefTVVVVQQjNsYYFbFurs65ehG5DZiDdnN90jm3WkRu9ubPAGajXVw3oD2VrgusLyIJwNeBmw5WTL96fTVr8kr3ml5ZW0+Uz0dM1P7nyxF9UvjF+SNbnX/fffexatUqli1bxty5czn33HNZtWpVU3fUJ598km7dulFVVcXxxx/PRRddRPfu3Zu9x/r165k5cyaPP/44l1xyCa+88gpXXml3kTTGRFZEr4Nwzs1Gk0DotBkhzx1wayvrVgLdw83ryiZMmNDsWoUHH3yQV1/VHrzbtm1j/fr1eyWIgQMHMnr0aADGjRtHTk7OoQrXGHMEO+yvpA7V2pn+6rwS0hJiyEqLj3gMiYmJTc/nzp3Lu+++y6effkpCQgKnnXZa2GsZYmNjm577/X6rYjLGHBI2FhMgyN79qw6S5ORkysrKws4rKSkhPT2dhIQEvvjiCxYsWBCZIIwx5gAcUSWItkUmQ3Tv3p1JkyYxatQo4uPj6dmzZ9O8yZMnM2PGDI499liGDh3KxIkTIxKDMcYcCHGR6t/ZAcaPH+9a3jBo7dq1DB8+vM311uSVkhIfRXZ6QiTDi7j2fFZjjAklIkucc+PDzbMqJohsP1djjOmiLEFg+cEYY8KxBBFw+NS0GWPMQWEJwmP5wRhjmrMEgVUxGWNMOJYgjDHGhGUJAojgdXL7LSkpqaNDMMYYwBIEYFVMxhgTjl1JDUSyCHHXXXfRv39/brnlFgB++ctfIiJ8+OGH7Nmzh7q6Ou69916mTWt5N1ZjjOlYR1aCePNu2Llyr8l9a+vx+QSi/Pv/nr2OgSn3tTp7+vTp3HHHHU0J4sUXX+Stt97i+9//PikpKRQWFjJx4kQuuOACu6+0MaZTObISRAcYM2YM+fn55OXlUVBQQHp6Or179+b73/8+H374IT6fj+3bt7Nr1y569erV0eEaY0yTIytBtHKmn7uzjNhoH/27J4ad/1VdfPHFvPzyy+zcuZPp06fz3HPPUVBQwJIlS4iOjmbAgAFhh/k2xpiOdGQliNZEuGZn+vTp3HDDDRQWFjJv3jxefPFFevToQXR0NB988AFbtmyJbADGGHMALEF4Ijmo7ciRIykrKyMrK4vevXtzxRVXcP755zN+/HhGjx7NsGHDIrdxY4w5QJYgDpGVK4ON4xkZGXz66adhlysvLz9UIRljTJvsOgjsOghjjAnHEoQxxpiwjogEsc+75nWioTYO1OF0Z0BjTOdw2CeIuLg4ioqK2jyAdvUqJuccRUVFxMXFdXQoxpjDyGHfSJ2dnU1ubi4FBQWtLpNfVoNPoLog9hBGdnDFxcWRnZ3d0WEYYw4jh32CiI6OZuDAgW0u89O/fUxibBT/vH70oQnKGGO6gMO+iqk9RIRGq8M3xphmLEEAPonshXLGGNMVWYLAShDGGBOOJQi0F1Oj5QdjjGnGEgTgk8PgQghjjDnILEEAIlgVkzHGtBDRBCEik0VknYhsEJG7w8wXEXnQm79CRMaGzEsTkZdF5AsRWSsiJ0YqTp+IFSCMMaaFiCUIEfEDDwNTgBHAZSIyosViU4DB3t+NwCMh8x4A3nLODQOOA9ZGLlYrQRhjTEuRLEFMADY45zY552qB54FpLZaZBjzj1AIgTUR6i0gKcArwdwDnXK1zrjhSgWovpki9uzHGdE2RTBBZwLaQ17netPYsMwgoAJ4Skc9F5AkRCXs/UBG5UUQWi8jitobTaItPsAshjDGmhUgmiHBj4LU8Cre2TBQwFnjEOTcGqAD2asMAcM495pwb75wbn5mZecCBWgnCGGOai2SCyAX6hrzOBvLauUwukOucW+hNfxlNGBGhjdSWIYwxJlQkE8QiYLCIDBSRGGA6MKvFMrOAq73eTBOBEufcDufcTmCbiAz1ljsTWBOpQEWExsZIvbsxxnRNERvN1TlXLyK3AXMAP/Ckc261iNzszZ8BzAamAhuASuC6kLf4LvCcl1w2tZh3UFkvJmOM2VtEh/t2zs1Gk0DotBkhzx1wayvrLgPGRzK+AF9Xv2OQMcZEgF1JDQg2WJ8xxrRkCQLw+ayXqzHGtGQJAhvu2xhjwrEEgV4HYfnBGGOaswSBDdZnjDHhWIJAezFZFZMxxjRnCQJrgzDGmHAsQaAXyll+MMaY5ixBoNdBWIIwxpjmLEGgbRDOMoQxxjRjCQLtxWTDfRtjTHOWILDB+owxJhxLEGgvJksPxhjTnCUIAr2YLEUYY0woSxAEGqk7OgpjjOlcLEEQaKS2DGGMMaEsQaCD9VkvJmOMac4SBF4jtZUgjDGmGUsQ2FAbxhgTjiUIrA3CGGPCsQSB14upo4MwxphOxhIENty3McaEYwkCa4MwxphwLEHg3XLUEoQxxjRjCYLAdRCWIYwxJpQlCLwSREcHYYwxnYwlCGy4b2OMCccSBIErqTs6CmOM6VwsQaDXQYAN+W2MMaEsQQCCZggbsM8YY4IsQWAlCGOMCSeiCUJEJovIOhHZICJ3h5kvIvKgN3+FiIwNmZcjIitFZJmILI5snPpoJQhjjAmKitQbi4gfeBj4OpALLBKRWc65NSGLTQEGe38nAI94jwGnO+cKIxVjSKwAOOvsaowxTSJZgpgAbHDObXLO1QLPA9NaLDMNeMapBUCaiPSOYExh+QIJwvKDMcY0iWSCyAK2hbzO9aa1dxkHvC0iS0TkxtY2IiI3ishiEVlcUFBwQIEGq5gsQxhjTEAkE4SEmdbyCNzWMpOcc2PRaqhbReSUcBtxzj3mnBvvnBufmZl5QIEGG6kPaHVjjDksRTJB5AJ9Q15nA3ntXcY5F3jMB15Fq6wiIlDFZCUIY4wJimSCWAQMFpGBIhIDTAdmtVhmFnC115tpIlDinNshIokikgwgIonA2cCqCMYKWC8mY4wJFbFeTM65ehG5DZgD+IEnnXOrReRmb/4MYDYwFdgAVALXeav3BF71ehdFAf9yzr0VqVgDJQjrxGSMMUERSxAAzrnZaBIInTYj5LkDbg2z3ibguEjGFsoaqY0xZm92JTUh3Vw7OA5jjOlMLEEQ7MVkJQhjjAmyBAFNdUyWIIwxJsgSBMEShNUxGWNMkCUIbLhvY4wJxxIEIVdSWxHCGGOaWIIg9ErqDg7EGGM6EUsQ0DQiVKNlCGOMadKuBCEi3xORFG9IjL+LyFIROTvSwR0qTVdSG2OMadLeEsS3nXOl6JhImeiQGPdFLKpDLJAerJurMcYEtTdBBI6hU4GnnHPLCT9Ud5fk8/aC1TAZY0xQexPEEhF5G00Qc7yRVhsjF9ahFbyjnGUIY4wJaO9gfdcDo4FNzrlKEelGcOTVw4aVIIwxJqi9JYgTgXXOuWIRuRL4GVASubAOrWAjtWUIY4wJaG+CeASoFJHjgB8DW4BnIhbVIWbXQRhjzN7amyDqvXs3TAMecM49ACRHLqxDy+4HYYwxe2tvG0SZiNwDXAWcLCJ+IDpyYR1aTUNtWH4wxpgm7S1BXArUoNdD7ASygP+LWFSHnA33bYwxLbUrQXhJ4TkgVUTOA6qdc4dRG4Q+Wn4wxpig9g61cQnwGfAt4BJgoYhcHMnADqXgdRAdHIgxxnQi7W2D+ClwvHMuH0BEMoF3gZcjFdihZI3Uxhizt/a2QfgCycFTtB/rdnpNJYgOjsMYYzqT9pYg3hKROcBM7/WlwOzIhNQBrARhjDF7aVeCcM79SEQuAiahh9PHnHOvRjSyQ8jaIIwxZm/tLUHgnHsFeCWCsXSYYC8myxDGGBPQZoIQkTLCV80L4JxzKRGJ6hATbKgNY4xpqc0E4Zw7bIbTaIuVIIwxZm+HTU+kr6SpkbpjwzDGmM7EEgR2wyBjjAnHEgR2HYQxxoRjCQK7ktoYY8KJaIIQkckisk5ENojI3WHmi4g86M1fISJjW8z3i8jnIvJGJOO0wfqMMWZvEUsQ3j0jHgamACOAy0RkRIvFpgCDvb8b0TvXhfoesDZSMQaI2HDfxhjTUiRLEBOADc65Tc65WuB59I50oaYBzzi1AEgTkd4AIpINnAs8EcEYgaZOTFaCMMaYEJFMEFnAtpDXud609i5zP3r/68a2NiIiN4rIYhFZXFBQcECBBhupLUMYY0xAJBOEhJnW8ggcdhnvpkT5zrkl+9qIc+4x59x459z4zMzMA4kz2EjdZioyxpgjSyQTRC7QN+R1NpDXzmUmAReISA5aNXWGiDwbqUCtm6sxxuwtkgliETBYRAaKSAwwHZjVYplZwNVeb6aJQIlzbodz7h7nXLZzboC33vvOuSsjFah1czXGmL21ezTX/eWcqxeR24A5gB940jm3WkRu9ubPQO8pMRXYAFQC10UqnrYEBuuzK6mNMSYoYgkCwDk3mxY3FvISQ+C5A27dx3vMBeZGILwmPl9gW5HcijHGdC12JTU23LcxxoRjCYKQK6mtmdoYY5pYgiD0SuoODsQYYzoRSxAEezFZI7UxxgRZgiD0fhAdHIgxxnQiliAIXs5t10EYY0yQJQiCJQhrgzDGmCBLEFgbhDHGhGMJgtAE0bFxGGNMZ2IJAhvu2xhjwrEEgbVBGGNMOJYgsNFcjTEmHEsQWBuEMcaEYwkCG+7bGGPCsQRB6GB9xhhjAixBENJIba3UxhjTxBIEoY3UHRuHMcZ0JpYgCA73bfnBGGOCLEFgQ20YY0w4liAIvVDOEoQxxgRYgiCkF5PlB2OMaWIJguB1ENZIbYwxQZYgCGmDsGZqY4xpYgkCu+WoMcaEYwmCkOsgrI7JGGOaWIIg9H4QxhhjAixBoL2YRKCuobGjQzHGmE7DEgR6JXX3xBgKy2s7OhRjjOk0LEF4MpJiKSir7ugwjDGm07AE4emREkdBWU1Hh2GMMZ2GJQhPZlIs+ZYgjDGmSUQThIhMFpF1IrJBRO4OM19E5EFv/goRGetNjxORz0RkuYisFpFfRTJOgB4psRSW11hXV2OM8UQsQYiIH3gYmAKMAC4TkREtFpsCDPb+bgQe8abXAGc4544DRgOTRWRipGIFLUHUNTiKq+oiuRljjOkyIlmCmABscM5tcs7VAs8D01osMw14xqkFQJqI9PZel3vLRHt/kTu1n3kZ44pmAVg7hDHGeCKZILKAbSGvc71p7VpGRPwisgzIB95xzi0MtxERuVFEFovI4oKCggOLdMvH9KzaCEC+9WQyxhggsglCwkxrWQpodRnnXINzbjSQDUwQkVHhNuKce8w5N945Nz4zM/PAIo1NJZEqAPJLrQRhjDEQ2QSRC/QNeZ0N5O3vMs65YmAuMPmgRxgQm0x8o9ZobS+uithmjDGmK4lkglgEDBaRgSISA0wHZrVYZhZwtdebaSJQ4pzbISKZIpIGICLxwFnAFxGLNDaZqLpyBmYksjqvJGKbMcaYriQqUm/snKsXkduAOYAfeNI5t1pEbvbmzwBmA1OBDUAlcJ23em/gaa8nlA940Tn3RqRiJS4FKgo4JiuVxTm7I7YZY4zpSiKWIACcc7PRJBA6bUbIcwfcGma9FcCYSMbWTGwyFG3k2OGpzFqeR2F5DRlJsYds88YY0xnZldSgCaKmjGOyUgFYmWvVTMYYYwkCIDYFasoYlZVKbJSPuevyOzoiY4zpcJYgQBNEfRWJUY6zRvTk9RU77N4QxpgjniUI0EZqgJoyLhydxe6KWt5ds6tjYzLGmA5mCQK0DQKguoRTh2YyKCORP7/zJfVWijDGHMEsQUAwQdSUEe338ePJw1ifX849/15pScIYc8SyBAHaBgFQUwbAOSN7cvuZg3lpSS5/effLDgzMGGM6TkSvg+gymkoQpYDeo/oHXx9Cfmk1D3+wEZ8I3z9rCD5fuKGjjDHm8GQJAvYqQQT88oKRNDQ6/vr+Bsqq6/nF+SMQsSRhjDkyWIKAkF5Mpc0nR/v5w8XHkhofzRMfbWZ9fhk/nTqCEX1SOiBIY4w5tKwNAkJ6MZXuNUtE+Om5w/nthaNYmVvC1Afnc8fzn1Nid54zxhzmrAQBEBUHST1h1+qws0WEK07oz3nH9uGxDzfy6LxNfLShkDOG9WDa6CwmHZ1xiAM2xpjIsxIEgAgMOg02zYXG1ru1psZH86NzhvHCTSdywsDuvLVqJ1c8sZDLH1/AH+esY3dF7SELucvatQaevRhqKzo6EmPMPlgJImDQabDiBchfDb2OaXPRcf3TGdc/nZr6Bv7yznrmfVnAw3M38NTHmzlnVC+OyUrlrOE96dst4dDE3pWsfBE2vAM7V0G/Ezo6mgNTvBXqayHj6I6OxJiIshJEwKDT9DHno3avEhvl5+4pw3jzeyfz9h2n8PURPfnwywJ+9foaTv2/D5h8/4f8bvZatu2ujEzMXUFti8++5VN93LP50MdysPz3Tnj5ur2n19fC63dA7pJDHpIxkWAliIDk3hCbCkUbD2j1wT2TuX/6GJxz5O6p4qXF2/h8WzFPfrSZJ+ZvIj0hhgEZiUwb3Ydvjs0mKfYg7frdm2DGKfCdd6DH8PatM/9PkNYfjrn44MTQmopC+MtIuPRZGPx1qKuGvKU6b/c+EoRzWvW3L40NsHwmzL0Pbv4I4tO+ctj7tHsjlObtHeNHf4YlT2lvuIufhO1LoNdx4Lefmema7D83QATS+8OenK/4NkLfbgn84OyhAOwsqeb5RVvZVVrDitxifv6f1dz35hcM65VMSnw0vVPjuOakAfRNTyDxQJLGuregtgw+fRimPdT6cjtXQX0NZI+D936t0yKRIB4/E46bDhNu0CRQXw15n2uCyFsKDV47TVv7+aVroXQHXD9HX7eWLDZ/CP+8EBrr9XXBuvZXWy19BrInQI9hrS/T2ABlOyE1KzjNOSjJ1c9Vng/JPXV6fQ189JfgMmv+Ay9eDec/COOuaV9MxnQyVsUUKn3AV04QLfVKjeOOs4bw+28ew39vP5lXbzmJb43LJjbKz56KWv6zLI/J98/n2F+9zQUPfcTtMz/nl7NW8+vX17C5cD8acku2tT3/rbvh9e9BXVXIOrkhz7fDU1P1rPfzZ7W6ZH9Vl8L2xbDWu/V4RYE+7loNb94FW73qpfSBbVcxrX4Vti2ArQu01PH7bPj0b3sv9+H/aXJI6qWvi7c0n7/qFS1ZBJTmQeVu3QezvgsLH2n78yx9Gh4cAxVFwWnl+ZocoPln2LU6OL0kFxZ47128NTjtzbs0kXxVRRvhka+12usurGemwbu/+urbbk19Dcw4GWb/WBNrS1V7YPVrkdt+wJZP9f/HHBRWggiVPgC+fEt7MvkikzvH9EtnTL/0ptc7S6qZ92U+OUWVrNpewscbCimvqcc5eOqTzfRNT2Bor2QSY/wkxUVx1vCejOufTlJslF7VXbZD36hwffMNle6A/DVw9Jn6umij9hwq2R5cZv07+mMady18fD/sWA6zboddq/TAdvpPvOXe1flX/huiYlr/cIEktX2pHiQqvBsvrZ0FrlG7EkfFQf9JsP7t5uvWVekBNi4tOG3hDBh0OtSWw/v3wom3BOflf6EliK//GibcBL/tCXtaJIiXv62PE2/RiyGfvgB6joBT79LpBfsYZ2vzfGiogZ3L4agzmn9G0JOJDe9CWr9gyWjAyZC7GOq9RBz4fp6/XPfv0CnB9q6WSvMAgZTebce1fCbsWgn/uQ2+8y74/K0vG7i2Z8une40UQNlOmPE1rQLsN7Htbe7L7s2wc4X+ZY3VUmSo2T+ClS9BxifQc+RX21ZbPvqzJs6RF0ZuG0cQSxCh0gfoD71sR/NqhQjqlRrHpcf3a3pd39BIg3OUVNXxr4Vb2VhQwYJNRQhQWdvAswv0jDQhxk/PlDh+07CSrwGUbmfpihVkZh/N7opasj79Pd3XPovclQO+KCjL0w0UrA1ufMULelZfU6oHL4By7z4YO1fqo3Pw7i80aexcqVVUrQmcLdeWQ/5aKPdKEK4x+N6Zw6HbQE0eVXsg3kuWb94FWz6B60MSR+H64EE8oXvzbQXaMoZOheg4LUUU5+i0L2bDxveCy755F/Q+DorWQ3UJFG3Q6flr4K17YMxVmjha2r44uC8CCSK0lFK0UUs26QMgawzEd4P+J0HOfJ0fnahJpHJ3cP8ufwGWPK1JeeApzavOXrpOv6vr/ts8jl2rodtR+jkB1r4B0Qm6D3IXtX1wf/k6Lck11GgVnHN6ovD46brvKwq0tLOvBFFbob+N+PTw81uWploq904Wdqw4eAki/wtY/Hc45/fBdp6S7Zr4vspJ3urX4N1fws3zgxfRHqEsQYRKH6CPe3L2P0FsmgtR8a3XgTsHX86Bo89qvdGyupSo6hKi0vrSI9nPHWcNaTa7pr6BeesK2FxYwa7SGnaVVZO2uZAiUulOCW++8AiPN5wHwCsxn5Dha+CBZ2ZSn9CDH3rvseyTOYwGqtOHEBeo8sn7PLiRQLVQRaE+bnxfkwPowSg0QVQUahVPcqCKZ2twXu5nwRJEqG6DdB+8/xv4+EE46xc6fcsnegAP9CJL6qUH8vpq3a8lW/VsOC5Fz1bz14I/RqurQM/i92zR/fz8Zc23ufxf+gca0+YPvf1dDAv+pts498+w9nXIHAofPwAn3hr8PDu9z1/wZbDqKD5dk1BdhXaNrtqjZ86pfXW+PxaGTYWcj7WkFhoLwOp/62fMGAzXvA4NdXrA90Vp6StQKqjcDY+eChNvhrPv1RgK1sJJ34VP/qrJK9zBffN8qNqtsZfv1Gm15VrVteVjKAwpPYnoduLT9Xm4Np83f6xVj9e9qUkwYPFTuj8D7UAJGdpxoqXAgXb7Yhh92d7zD8SKF+Czx7S0kuX9X5ZuB9eg/8eB9qH9tXCGJrxFT8DXvn9wYu2irA0iVDfvYBN6wGwP57SO98mzW19m8zyYeSkse7b1ZV7/Htw/Knhm+onX6Pzer2HZTGKj/Jw9PJObhlTw8ylH8/DlYxmVXEm3kWdS22cCd3RfyP9+cxSPXzmG46K1fSGtYCnbNq5p2kSj1830HwVDm226Hj9r5Kim1+U7N/DQ++vJ/e99VMRmUhmbyZYVc5m/dAW5G1aRv3Ud7pFJuEdOCvb8Kt6qB/OEDK1mCSSbUN0GQp/RcOyl2rBeUaRVH4Gz+uXP62P/k4J1+qMu0sdl/9ID4l/HwicPQvfBwWSb3l/P7ncsa749XzSkeiU08f7dV7zUfJmNH2gifPEqeHgCLHtOG8pBP0ugNPXK9ZokAXqP1vaagLI8rTpKzfbmHwsZQ3T6toVatTbgZJ037DyY8n+aoHLm6/7LX6Nn6HWV+vqTv8KSf+i6jXV6cK6rhk//qu914ne1Om7XKq1OBN2Xu7zves498Mb3g8khoGCd7uOo+OC01a/CHwbC0+fDwsfgD4Pgs8f1hCbQnrD6P/r4n1u9z7tLS4hv/liT3dpZOuhl9vhgDzXn9Gz87Z8F2/YWPaGfzTn9K9uppbpQ7/0GVv27+bSijbp8s8/yhT7mfKyPtRW6TyFYtdceoSc2ADGJ+vjJQ21eOItz2pa1aW77t9XFWAkiVPpArR+f9wc9KIWrC17zH/3hXvFy8CwvcABpy4Z3g+uPu1avD8iZD0PO0edlO4Jntm/cEexGOeFG7ZYKWv+9+ElddvL/wgk3QdkOZMhkYo46g5hZt3Fp73zt6tmgdeDX9N3JVUeNBO8kdqxvA3XxmYwddQYser0pvO3R/amNy4YyPdgn1e+mzwd3kO3/jN/XXcaxvo2cVf0uWdvnUEkcqxoHEOcrpQEfGx/5Ds8kXsud1XNIiu5FUWw26WvnUxrVjf4Shd/VUx2bQVxNIXn+3lQXlNN97K2krniB+mUzicoeC3g//nVe9Ur/k/TAAzDqm5pY37pL2zECVVaZIUkurb/WcT97UfP9fu1/9aD15o/1APbJX6GmBFKy9GwTvLPFvwfXScnSM+w+Y7Ua6JMH9Yw83zv4jrlKSwubPmi+rfHXBw9MfcYGS6RrX4eeo7SaK2c+DD9fz3qPPlOT3aYPtOQQsOIFmP/H5u9dtUerU5bN1F5RyT31PZf8Az5/Di54EOb8RJPtpc+G+Z8U3cezbtMYT/uJxpczHz7/py6SM1//fFEw+06dljUOLn9Re8oFSgdL/6nvE2h7iU7QxJbWX0uIm+bp/+6iJ4LvE+rtn+n+LPxS933PY2DQqRr7SbcHP3tyb0jM0CTy9Hlw4aPN2zbyverSnI/0dxLavvbCVRrbpNu1NNiajR/AP7+h3ZLrquG4y4JtWZWFetIROHEM9flz+jtb+oz+P7bWrlS4Hj78I5x2d/j36eSsBBFKRLslNtToP3bLMxbQH+TG97WHTUBog2vLC8MCNnoHk03ztDg/7z741yX6Q3nv19pYWFuuy2xdoGdce3KC1TsAH/xWfzCgP+SaUv1hpvSGkd/QH+rn/wzWd/edCLmL8RVt0Gs8RBNadHpfJkw8VZdJ7AFA/xETGT3c6/IZmwrAN/0f0TDwdG6583d87YJv40/pyZ6B55IilZzkX8PKrEvY0GsKoxrW8GDJ7fSrWU9hRS2zCvqQUbOV1Ioc5jccw4v1p3Jv+fkA3Pl+BWf8aR7HzchlmTuaTW8/wqw3NSnM909o+qhPbNXqgQZfNEv8oygY9E2dEWgjAVxKFnsqaqmoqccN+JpW2fQYATd9qAcv0AOWzw/n/kmrs8Z7F7j1nwQjpsFU72C07r+aDK55A26ar/Xa17yuB5e0fnqAaqyHq17V7sShjaDTHoZrZ0NMgi47ZIp2Ie7mlcgq8rXUdPRZOn/w2cHY0vrr/0be5xCXqiWe+X/UKqpBp+tyKdn62d77jZYmjv+OTs8YrI+NdfDaLRCTrAl0ZosGYtBkkNpPk8PAU7Xq5LhLod+JwVgufwmyj9fP/+05cN79Wkqadbsuc9J39XHWbfoYOPMOHIBrSvV96qtgzk+111SfscEYTrhZk9ex0/U31G0Q9BmjDe6fPqS9xh4KqcKc8xN4aLzue9CTq4DaSv19iA/Wz9GS984VwfklW7X0tPBRjeWte7R67S+j4B/nBUu9K73S5Cs3wH9ugc1z9XMFSnuhibZkuyb7pc/oss9frtPb6uywfCaseB4eO23fw8vsWhOs2u0krATRUsbRcNo92jD76cN6QAkUOWsrg8XZta/DgEn6fN2bwfX35DRv8Ny+VOt2d63SBtV1s7WkUOz1hvnyLVj1sh7oQQ9SgZIEwBdv6OPNH+kZXFIPPRhsfD/Ycym5t9bxjrxQi+a7VunBZuzV+o+85jUdPmSLV7/fZ4yWlhK6a0lp8zwtyQTqjoecowfUqDj8UTGkAhx/GRx/GZmNDfojK8tj0jdv02T0SrDK5qiTLuSHQ8+Bp1+gO8WcOmEce07/PccUl7N29TFc3XsKl9Q7dpVWs2XjJUzb+jtSd/6LHdKDpb2nc3LuZwA8sMzxbb/wZX0vLnp0MXAxZ/gG8WTMH1nb2I/hvq385OM6Zn6gRaMon9Ar9REG1CdyzPJYLooaTF9/EX/+sID+3atIjosiOS6KxOF3k1d1HGQO5eRxxxEX7aNxy+ckrX5OzwIHegeGQI+p2CS46jV4cLS+7ufVv8enaw+q+HQYc2Xw+/JHw+VeNZlzesAu36WJ66jT4Y6QA46IXh/y+XP6/fWfpP8Hm+bC1+6AURfDw8drySO5p5Yeux8NmV4i7+4N9RGXplUrp9yp3/NTUyA6XksdoImj20D4xgxNED1HBavmAu8x8kIYcrb+BfSbqP/ngVLdcdP1dwEw6Q7t2RYVD+O/rV2O0/oFz5IXPKzJ4VtPwV/HaxLrMUJLT0PPhfMf0Eb36lL44xAv8d0Q7Ho87lo9GQuIS9X/+cBBtvBLwMHJd2ppe/nz2qsrVOYwrYb61KuqXfmyJuuSbZokrn1Df19R8cFeZ4uf1BPEIedoW83OlTDiAm3Lefq84D4NtWs1vHqzliyXPKW9//KWaVVioP2qulg7hBx91t7rg5ae/n62lirPvx+eOhdO/bGe+IEmjv/+QBv7x1+vJyCfPqSltP/5JHiMOsgsQYRz4m1a1/z2T+GD38E1s7SaIucj/edJ7KE/nMm/14Pq9sX6A1v9qldk9hJE0UZ4/IxgI+5p92iDZd7SYFXEwhnBBj6AYy5pniBWv6Znk5nDgz/qvhP0rOTJc7yeM16iOul2TTjbl+jBIJDAqkvgqNP0cddKOPPn2sPjxnmQ0A1ivGsFlnkNqMm9gvfIaMnnh5N/oGe8GYP1QASAwPeW40/J0rYDfyw01CDxqXRLjKFbYjfIup5m13qf9D24/3F6VhTAlD/yvfHXwq9/AMCK31xAw4MD6dtzHA+NGENibBR+N47Vy2pYkHwOyxvLiXN9+Vl6Ag2NjuKqOnYUV7FuVzmPztvIO5zDAN9oPvx4C7V73Vc8HchH3nwH5yCKc/huWjLzFo+AlR+TkRRLZnIs3RNjiI+J4ugeSdROXUBjRRENq4rISIolOS6KoRNuIy567y6mzjntgiyiyeW1/wmWGloae43+yOurYPQVMGSy/o8FfvDXvaVtGXmf68Fr2HnBBuQJN2pbhz8WPvOqX6Ji4a4cPZn5ywhNUle8qNVryT33brjtd4KWlAL/Qy1Ne1jbJvwx+n/x9d9oSWHCTZogeo6AlD5aDdV7dLD6b8JNMPUP+jyph1bnBTp++Hzg83pkxaXogbChThNcdYlWLyZmaILofZzGUFGoVUEvXq2l8ECvsmMugcwhEJOk+yDUpDvgtZv1+XGX6dl8SjZc/oIm0SfO1O1d9Hf93W75VH/XABlDtY1r50rdh/+5Rdt+rp2tJZNdq4NVv3UV+t7LZ+rrbZ9p1dSw87RUM/x8vaB1/TuaJFP6NO8IUFup1Xa1Zdru895vtOPDwkeDCeLLOVqC6jYI/v0dnb9ghv7fLJ8ZLFUeZJYgwvFHwcVP6Zn3nJ9ql7drXtcfRHy6HiDfulsTwMqXAIFTfqQJYvdmbWxOyfb6xjtNBr1H64+51ygtVeSv0feqLtGqiKSeeoYx/Hxtgwgkjd0bdd3Qnk+BXiuN9XDZzGBbSY9hWr2y7bNgFUhyb93+UWfAuOs01sBwFGl9m3/uQCJL6dP2/plwQ/B5arZ+1uRe2lAM4E+C6c/BcxfrD7w10XFwyo+1zn3s1Zp8ps8E14iIEHX1ayTFJnNeQrfgOsPuZV+dJJ1z1DY0Igh+n7CjpIrK2gb2VNRSVdfAsF4pFFXU8N7afPw+ITbKx39XZpCeEEN1XQM5RRUsytnNnspw9/xY1vSsW2IMx2Slsm1PJcmxUdTUN1JWXc+u0mrG9U/nkvF9qalPYlPWIxTNKeLoHjUclZlIlM/HgIxE4qJ9ZGSMhD4nEFOyGRl8Nvj8iD+R+gbdB/7+XhXQoFN1XwWqyECvSRl4ij4P7T0XHa9/GYO1Xj2011E4gfcIJ6U33LYo+P846fbgvCGTtUoK9Iw74J7c5t1D+4zRBNGyq3LAyT8IPr/QK0HUVuqJ2JirtFTkHBx1ZrAtb/0c3X6gmu2sX+ydIEZdpO0d3QbCOb/Tkv7oy/U3eM7vtKps9BW6nIj+fgPdo9P763Y3z9OSxI7lWh0ZOOnKGq8JIi5170b2QFfoQOm/70StMVg4Q7s4DzpVX1/zurY/vfEDbReLSdJq5sV/11Lh1k+0uiutn3a+iEmCWz+D56/Qq/ZjU3XeJw/pfoqKDb9/vwJx4erZu6jx48e7xYsXH9w3XfioNnAOv0B7apx3v9ZPPjROG/oWPKw/kitfgfv6Qa9jg/3gQQ/+e3Jgyv/qgfW/P9QzRtCeLMd/R8+oNn6gXT3P+Cn87SSoLAr2QDn5Tjjz/zWPa+0b2oC4r4uqXr5eqyzu/LLtC6pAG+ceHK0N8IEL7Npj60KtimnZv72+tu0L6zo5512PklNUSVJsFImxfipq6tldUUdReQ0vLcllV2k1fdMTKK2uIz7aT2p8NCnx0bz3xS627dZqi9goH+kJMewsrd5rGz6B7q6Y9OhaiuP6UlZdz4g+KazbWUZqfDTHD0hnYEYSIrC7opY+aXGcMiQTnwg+gdw9VdQ1OM4Y1gN/y3umr3xZL0Ace9Wh2F2tq9qjsRz/nfaNrxXQ2KBtDIF1ynZqVdawc/V6krPvhaTM4PJlu7QK6f17tfr19qXanhefrh0aqor1IOuP0oSzfalXAosOvsfHD2g17fVv64nW0+drac41wg+/0IQAuv4LV2rNwru/3Dv2zOHBa46unuVdc3N3sDpQfJpk8tdobEefpYl6zk+0lHDKj2HGJD3ufOsfWv3ki4Jvv6nVbJvn63Enb6meiJ3yYz12HAARWeKcGx92niWIfWio13+SrZ9o4+P05/TL/X22ZntftNYBZg7Rrq6b5gIC5/xWD/on3aZfeEq2JoLPn9PiKugZRLizt4WPaR/22nL9gU/5w74P7q0pL9D3Cu3x05bSPC112L23v5LGRseqvBLSE2LISovH5xNKqurYtruS6roGtu2ppLSqnsLyGvp3T2TR5t2UVtfRKzWOJVv20Cc1ntLqOrburiR3jyaaxBg/FbVhhrEAhvVKpl+3BFZtL6G6vpGRfVLYXlxFUXktCTF+4mP8HJedRu/UOKL8Pr7cWcaQnklkpsTR2OgY2iuZ6roGEmKi6J4UQ32DY+HmIo7ukcSJg/TMv7C8lthoH/HRfgSI8h8BfVzm/0mrh076rrZFtOSc1iwMO1/bNja8q20DY67UK/jn/wkueEhLdPXVeqFgntcu+epN+h7/82n4CzU/+osmn5RsKM2FibfC5N/tvdy/b9Tjzu2fH1BbhCWIr6o8X9sCxl0TLMa9dov2l//GjOCFPzuWa2+FvhM104dTXQIf/F4P/lP/GLw61phWVNTUE+33ERPlY2dJNR9+WUBcjJ+Gxkay0hLYuruSZxdsobS6jpF9UonyCZsLK8hIiqFPWjxVtQ2U19Tz0fpCquoaqG909EyJJb+sJmxHvZb6dounqLyWytoGYvw+ov2CT4RxA9KJ9vuIi/bz5c4yRmalkF9aQ3FVLeP7dyMlLorymgYKymvo1y2e/t0T8YlQXdfAgO6J9EyJpbK2AQcMzEikodGRGOunsqaBukatIkyOi2pq52lq2+nMVv1br14/90/7bhd45+daSjrnt+HnO6fVr2/epaWOCx/TnmctVe7W9wktTe0HSxCRUF2qX1qgO2XA2jd0Wu9jD00cxrRTXUMjPhHqGxuJ8fsorqyjrrGRxkZYs6OElLhoquoayC+toaSqjq+P6Ml7a3fxycYistLj6dctgdw9VdTUN1Df4Fi2rRjnoLymnl6pcSzfVkx2ejx90uJZlLObugan7SxJsewsqaa+cd/Hmu6JMZRV1zfrWJCRFEN6Qgw5RRVkpcUzsk8qaQnRfLGzjIEZiUT5hKTYKHaVaRtPWnw00VE+ausbm+4dP6pPKrsra8lMjmXb7kpKq+rw+YSVuSWcMLAbPp8mo+Oy0yipqmNQplbrbd9TRXpCDBlJMe0rMVXu1pPHc/908IbrqSjS7usn3BTSKeTg6bAEISKTgQcAP/CEc+6+FvPFmz8VqASudc4tFZG+wDNAL6AReMw598C+tndIE4Qxppmq2gZio3z4fEJDo8MnNJ3x19Y3squ0mkbniInykVNYSUF5DXFRetDNKdLuq4ty9pCZHMvQntrIXVJVR15xFYXlNfTrlkhecRWr8krIL61heO9kdpRUU9vQSHl1PT2SY8kr2budpy3pCdGtdEYAv/c5AGKifAztmUxaQjTFlZpckmOjKK2uo0dyHIMyE/ngi3yq6hoY1z8d56CwvIb6RsfUUb2I8vsYlRUs3W0qqKDBOYb2TGZor2SifELunipy91QSG+3jpKMyWLplD5sKK5gwsBvHZqcSG7V3NXNNfQMVNQ10Szzwtr4OSRAi4ge+BL4O5AKLgMucc2tClpkKfBdNECcADzjnThCR3kBvL1kkA0uAb4SuG44lCGOODKHVTY2N2mstLtpPXUMjpVV11Dc6Yvw+EmOjcDg+27yb9IQY8suq6d89kb7pCVTU1JOWEM3mwgqS4qLYWVLNyu0lZCTFsqmggsraeob2Sqa4UtuCVueVUFHTQGp8NA4oq64jKTaK7cVVbCqoYFz/dHqmxPLZ5j0kxvrJSIplT0Utm8IM2y+i17a3o1DVbB2/aK+7xNgoyqrr8QlU1DYwpl8aL950ItEH0C7UVoKIZDfXCcAG59wmL4jngWlA6EF+GvCM0yy1QETSRKS3c24HsAPAOVcmImuBrBbrGmOOUKFtET6fEOd14oj2++ietHd3z5MHB+rnU5umxXg97AZlJgHQIzmOY7PTDiie2vpGYqL2Pjg3NDpy91QiCGt3ltLY6BiYmciA7tqYvCG/nC92luGca6rGK66sY/bKHRybncbEQd2Yv76QnMIKahsaaWh0VNU1UFZdT3Kcdq3ulRJHQVnNASWHfYlkgsgCQgbPJxctJexrmSy85AAgIgOAMcDCcBsRkRuBGwH69esXbhFjjImocMkBtJqqv5cM+nVP2Gv+qKxURmWlNpuWnU6zaecft4/rkiIokv3UwnU3aFmganMZEUkCXgHucM6VhtuIc+4x59x459z4zMwDa8U3xhizt0gmiFwg9FLdbCCvvcuISDSaHJ5zzrUY+9cYY0ykRTJBLAIGi8hAEYkBpgOzWiwzC7ha1ESgxDm3w+vd9HdgrXPuzxGM0RhjTCsi1gbhnKsXkduAOWg31yedc6tF5GZv/gxgNtqDaQPazTUw0Mwk4CpgpYgs86b9xDk3O1LxGmOMac4ulDPGmCNYW91cj4DBVIwxxhwISxDGGGPCsgRhjDEmrMOqDUJECoAtB7h6BtC5bgjbfl05drD4O1pXjr8rxw6dI/7+zrmwF5EdVgniqxCRxa011HR2XTl2sPg7WleOvyvHDp0/fqtiMsYYE5YlCGOMMWFZggh6rKMD+Aq6cuxg8Xe0rhx/V44dOnn81gZhjDEmLCtBGGOMCcsShDHGmLCO+AQhIpNFZJ2IbBCRuzs6nvYQkRwRWSkiy0RksTetm4i8IyLrvcf0jo4zQESeFJF8EVkVMq3VeEXkHu/7WCci53RM1E2xhIv9lyKy3dv/y7xb5wbmdZrYvXj6isgHIrJWRFaLyPe86V1l/7cWf6f/DkQkTkQ+E5HlXuy/8qZ3iX0P6L1dj9Q/dJTZjcAgIAZYDozo6LjaEXcOkNFi2h+Au73ndwP/29FxhsR2CjAWWLWveIER3vcQCwz0vh9/J4v9l8CdYZbtVLF7MfUGxnrPk9H7xI/oQvu/tfg7/XeA3hAtyXsejd4Vc2JX2ffOuSO+BNF032znXC0QuG92VzQNeNp7/jTwjY4LpTnn3IfA7haTW4t3GvC8c67GObcZHQp+wqGIM5xWYm9Np4odwDm3wzm31HteBgTu795V9n9r8bem08TvVLn3Mtr7c3SRfQ9WxdTaPbE7Owe8LSJLvHtyA/R0zu0A/VEBPTosuvZpLd6u8p3cJiIrvCqoQBVBp469xf3du9z+D3N/+k7/HYiI37unTT7wjnOuS+37Iz1BtOe+2Z3RJOfcWGAKcKuInNLRAR1EXeE7eQQ4ChgN7AD+5E3vtLG35/7ugUXDTOvwzxAm/i7xHTjnGpxzo9HbKU8QkVFtLN6pYgdLEO25b3an45zL8x7zgVfRYuguEekN4D3md1yE7dJavJ3+O3HO7fJ++I3A4wSrATpl7K3c373L7P9w8Xe178A5VwzMBSbThfb9kZ4g2nPf7E5FRBJFJDnwHDgbWIXGfY232DXAfzomwnZrLd5ZwHQRiRWRgcBg4LMOiK9VgR+350J0/0MnjF2k1fu7d4n931r8XeE7EJFMEUnznscDZwFf0EX2PXBk92Jy2nNgKtozYiPw046Opx3xDkJ7OiwHVgdiBroD7wHrvcduHR1rSMwz0WqAOvQs6fq24gV+6n0f64ApnTD2fwIrgRXoj7p3Z4zdi+draDXFCmCZ9ze1C+3/1uLv9N8BcCzwuRfjKuDn3vQuse+dczbUhjHGmPCO9ComY4wxrbAEYYwxJixLEMYYY8KyBGGMMSYsSxDGGGPCsgRhTCcgIqeJyBsdHYcxoSxBGGOMCcsShDH7QUSu9Mb4XyYij3qDsZWLyJ9EZKmIvCcimd6yo0VkgTeg3KuBAeVE5GgRede7T8BSETnKe/skEXlZRL4Qkee8q4iN6TCWIIxpJxEZDlyKDpY4GmgArgASgaVOB1CcB/zCW+UZ4C7n3LHoVb+B6c8BDzvnjgNOQq/UBh2p9A70vgCDgEkR/kjGtCmqowMwpgs5ExgHLPJO7uPRgdYagRe8ZZ4F/i0iqUCac26eN/1p4CVvHK0s59yrAM65agDv/T5zzuV6r5cBA4CPIv6pjGmFJQhj2k+Ap51z9zSbKPL/WizX1vg1bVUb1YQ8b8B+n6aDWRWTMe33HnCxiPSApnsL90d/Rxd7y1wOfOScKwH2iMjJ3vSrgHlO72WQKyLf8N4jVkQSDuWHMKa97AzFmHZyzq0RkZ+hd/PzoSO83gpUACNFZAlQgrZTgA7lPMNLAJuA67zpVwGPisivvff41iH8GMa0m43masxXJCLlzrmkjo7DmIPNqpiMMcaEZSUIY4wxYVkJwhhjTFiWIIwxxoRlCcIYY0xYliCMMcaEZQnCGGNMWP8fuh5WLGOTSlMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################\n",
    "# REPORTS\n",
    "##################\n",
    "reports.plotHistory( hist )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46e4430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Projects\\VenusDenoise\\saves\\0100_1000-64-convsim-h\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rdelasheras\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "model.save(os.path.abspath(os.path.join('../../../saves/', MODEL_NAME)), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd185317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
